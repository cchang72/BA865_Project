{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA_865_Final_Project_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Final Project: Predicting Whether a Person's Eyes are Open or Closed**"
      ],
      "metadata": {
        "id": "_N6hQxGZbvUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Names: (Aleks Lazowski, Chris Chang, Jacinto Lemarroy, Shrey Sood)*"
      ],
      "metadata": {
        "id": "uS56ac1jcDMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Background:*\n",
        "\n",
        "According to the National Highway Traffic Safety Administration, there are approximately 100,000 car crashes, 800 fatalities, and 50,000 injuries every year related to drowsy-driving in the US (nsc, n.d.). To help combat this issue, our team decided to come up with image classification algorithms trained on 4000 images with two classes, open and closed, to detect whether a person's eyes are open or not. We hope this would be the first step to more improved algorithms to come in reducing the number of drowsy-driving fatalities not only in the US but worldwide as well."
      ],
      "metadata": {
        "id": "YptEWg90cLFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries and modules"
      ],
      "metadata": {
        "id": "0ZenjGgH2hjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense,Dropout,Activation,Add,MaxPooling2D,Conv2D,Flatten\n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "#from keras.applications import VGG19\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.vgg19 import VGG19\n",
        "model = VGG19(weights='imagenet')\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import os\n",
        "\n",
        "# import the necessary packages - MobileNet\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import cv2"
      ],
      "metadata": {
        "id": "cl896xXHZs2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import from directory and train, test, validation split"
      ],
      "metadata": {
        "id": "Z4kkIsS2i3aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our original dataset contained over 40,000 images of open and closed eyes.  We wanted to take a portion of that data to make it easier for our models to run and data to import.  We settled with 4,000 images, evenly split into open and closed eyes.  These 4,000 were randomly selected as well.  "
      ],
      "metadata": {
        "id": "AfkLWre3ISqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlF-gzj7CUi",
        "outputId": "f0a22cbb-b4f8-4b55-c8d1-74bf0faa8d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basePath = \"/content/drive/MyDrive/BA865/Final Project/train-2\""
      ],
      "metadata": {
        "id": "9h2LxLlq7CJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJb1iOra_KF4",
        "outputId": "371aba7b-0c4a-449c-bc66-0f7852eb4fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(basePath, output=\"output\", seed=1337, ratio=(.8, 0.1,0.1)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "QVcpE831_M0S",
        "outputId": "a1c46c94-3b28-4f2b-b305-9e894d3f440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-473130a27a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplitfolders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplitfolders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(input, output, seed, ratio, group_prefix, move)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`ratio` should\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_input_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_tqdm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mcheck_input_format\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_absolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf' Your relative path cannot be found from the current working directory \"{Path.cwd()}\".'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The provided input folder \"/content/drive/MyDrive/BA865/Final Project/train-2\" does not exists."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using splitfolders we organized the data into training, test, and validation splits on the file system. The labels are inferred from the folder names. "
      ],
      "metadata": {
        "id": "oOLbP_L8-5c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import train and test data\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "base_dir = \"/content/output\"\n",
        "  \n",
        "train_dataset = image_dataset_from_directory(\n",
        "    base_dir + \"/train/\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32, shuffle=True)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    base_dir + \"/val/\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32, shuffle=True)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    base_dir + \"/test/\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "b50VMu8C7Eb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "     print(\"data batch shape:\", data_batch.shape)\n",
        "     print(\"labels batch shape:\", labels_batch.shape)\n",
        "     break"
      ],
      "metadata": {
        "id": "Mi9zJf8llnC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "dQ4gIElgmMkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we delve into what our data looks like. We look at examples of the images as well as the class types. We visually represent the distribution of the classes as well as the data split into train, test and validation. As you can appreciate, our data is evenly distributed between the two classes (open and closed eyes) and amongst the data splits."
      ],
      "metadata": {
        "id": "fHe_u--O9Imo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What our dataset looks like\n",
        "plt.figure(figsize=(10, 10))\n",
        "class_names = train_dataset.class_names\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(32):\n",
        "        ax = plt.subplot(6, 6, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "UcSb8WpldR1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(class_names)"
      ],
      "metadata": {
        "id": "UMlNfaJ4e3tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data visualization of class types\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(\"Data\",4000, label=\"Data\",color=\"y\")\n",
        "ax.bar(\"Closed_eyes\",2000,label=\"Closed_eyes\",color=\"g\")\n",
        "ax.bar(\"Open_eyes\",2000,label=\"Open_eyes\",color=\"b\")\n",
        "ax.legend();"
      ],
      "metadata": {
        "id": "At9Xla75mV3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data visualization of data split\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(\"Data\",4000,label=\"Data\",color=\"y\")\n",
        "ax.bar(\"train\",3200,label=\"Train\",color=\"g\")\n",
        "ax.bar(\"test\",800,label=\"Test\",color=\"b\")\n",
        "ax.bar(\"val\",800,label=\"Test\",color=\"b\")\n",
        "ax.legend();"
      ],
      "metadata": {
        "id": "O1gKK_RMnC26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Our Model: Binary Outcome NN model"
      ],
      "metadata": {
        "id": "9geBaCPMnTXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we implemented a dense neural network model with a binary outcome in order to test its accuracy when predicting image classification"
      ],
      "metadata": {
        "id": "uG57WowjT5vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "  \n",
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling2D()(x) #before it was flatten\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TSDXI9-4nWS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "CNTyCnuyntZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"drive/My Drive/Teaching/Courses/BA 865/BA865-2022/Week 5/convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]"
      ],
      "metadata": {
        "id": "8hi2fWR2n2Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50, \n",
        "    batch_size = 100, # *default is 32\n",
        "    # INC adjust bs.. one epoch will be less iterations\n",
        "    # wont learn 100% acc\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "zt0SnSl6qiIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfASbQht93rn"
      },
      "source": [
        "Let's plot loss over training iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLs620wf96hj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['val_accuracy'],c=\"b\")\n",
        "plt.plot(history.history['accuracy'],c=\"r\")\n",
        "\n",
        "plt.legend(['Validation Acc','Training Acc'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTAotbse9ctY"
      },
      "source": [
        "Our model does fairly well in reaching high accuracy of above 90% with a few drops in accuracy but continuing to increase with the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk2U9j_G8-w_"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\"drive/My Drive/Teaching/Courses/BA 865/BA865-2022/Week 5/convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset) \n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Model"
      ],
      "metadata": {
        "id": "yejwCgTOabMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a pre-trained model (VGG19) to analyze the potential impact and improvement on our previous dense model.  VGG is one of the most popular pre-trained models for image classification.  Furthermore, we added Imagenet as a weight.  This VGG model was meant to train a model that can correctly classify an input image into 1,000 separate object categories.Â  Due to the background of this model, ImageNet challenge is considered the de facto benchmark for computer vision classification algorithms which is why we chose it to test our accuracy and predict open and closed images. \n"
      ],
      "metadata": {
        "id": "RSRek227ae43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model =  VGG19(include_top=True , weights='imagenet')\n",
        "for models in vgg_model.layers:\n",
        "  models.trainable= False\n",
        "vgg_model = keras.Model(inputs=vgg_model.input, outputs=vgg_model.layers[-2].output)\n",
        "vgg = keras.Sequential()\n",
        "for layer in vgg_model.layers:\n",
        "  vgg.add(layer)\n",
        "vgg.add(Dropout(0.2))\n",
        "vgg.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "873w0g1ojFQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg.compile(optimizer=keras.optimizers.Adam(0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "DnH4vHLjjH5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit our model\n",
        "history = vgg.fit(train_dataset,validation_data=validation_dataset,epochs = 3)"
      ],
      "metadata": {
        "id": "GhW7CBmIjLev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate test_data\n",
        "vgg.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "l-RUdLsElitq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['val_accuracy'],c=\"b\")\n",
        "plt.plot(history.history['accuracy'],c=\"r\")\n",
        "plt.legend(['Validation Acc','Training Acc'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f0HVC8OkNJuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNet"
      ],
      "metadata": {
        "id": "mqEV41fAsDI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 7\n",
        "BS = 32"
      ],
      "metadata": {
        "id": "jsa9R97ssGJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTORY = \"/content/drive/MyDrive/BA865/Final Project/train-2\"\n",
        "CATEGORIES = [\"Closed_Eyes\", \"Open_Eyes\"]\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DIRECTORY, category)\n",
        "    for img in os.listdir(path):\n",
        "        img_path = os.path.join(path, img)\n",
        "        image = load_img(img_path, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = preprocess_input(image)\n",
        "\n",
        "        data.append(image)\n",
        "        labels.append(category)"
      ],
      "metadata": {
        "id": "2ArYq9WfsIdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "U2yGw0NWsLAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, validX, trainY, validY) = train_test_split(data, labels,\n",
        "    test_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")"
      ],
      "metadata": {
        "id": "ckULKt77sOXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "baseModel.summary()"
      ],
      "metadata": {
        "id": "Won-6aFZsQwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"sigmoid\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers[:65]:\n",
        "    layer.trainable = False\n",
        "for layer in baseModel.layers[65:]:\n",
        "    layer.trainable = True\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(validX, validY),\n",
        "    validation_steps=len(validX) // BS,\n",
        "    epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "_WpQtv-fsTan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"plot.png\")"
      ],
      "metadata": {
        "id": "Ul5oLCEDsWN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting training accuracy & validation accuracy\n",
        "plt.plot(H.history['val_accuracy'],c=\"b\")\n",
        "plt.plot(H.history['accuracy'],c=\"r\")\n",
        "plt.legend(['Validation Acc','Training Acc'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gx96JjODsYok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Prediction"
      ],
      "metadata": {
        "id": "iTKfdmrga5RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will upload new images to test whether our model can correctly identify eyes close or open.  We used images from the dataset and from google to test if the model can accurately predict variation in images."
      ],
      "metadata": {
        "id": "v-SV7zxkbCc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense,Dropout,Activation,Add,MaxPooling2D,Conv2D,Flatten\n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "#from keras.applications import VGG19\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.vgg19 import VGG19\n",
        "model = VGG19(weights='imagenet')\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import cv2"
      ],
      "metadata": {
        "id": "Q8UQ4CqRXbtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input"
      ],
      "metadata": {
        "id": "9SKeq6_i1_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image 1\n",
        "image_path = \"/content/output/test/Closed_Eyes/s0014_00008_0_0_0_0_0_01.png\"\n",
        "new_img = image.load_img(image_path, target_size=(224, 224))\n",
        "img = image.img_to_array(new_img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "prediction = vgg.predict(img)\n",
        "prediction = np.argmax(prediction,axis=1)\n",
        "print(prediction)\n",
        "print(class_names[prediction[0]])\n",
        "plt.imshow(new_img)"
      ],
      "metadata": {
        "id": "PnLp0j9LrhOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image 2\n",
        "image_path = \"/content/drive/MyDrive/BA865/Final Project/test/closedeye.jpg\"\n",
        "new_img = image.load_img(image_path, target_size=(224, 224))\n",
        "img = image.img_to_array(new_img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "prediction = vgg.predict(img)\n",
        "prediction = np.argmax(prediction,axis=1)\n",
        "print(prediction)\n",
        "print(class_names[prediction[0]])\n",
        "plt.imshow(new_img)"
      ],
      "metadata": {
        "id": "nPiLuupgnlpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image 3\n",
        "image_path = \"/content/output/test/Open_Eyes/s0014_07731_0_0_1_1_1_02.png\"\n",
        "new_img = image.load_img(image_path, target_size=(224, 224))\n",
        "img = image.img_to_array(new_img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "prediction = vgg.predict(img)\n",
        "prediction = np.argmax(prediction,axis=1)\n",
        "print(prediction)\n",
        "print(class_names[prediction[0]])\n",
        "plt.imshow(new_img)"
      ],
      "metadata": {
        "id": "uGbblLEOCu3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image 4\n",
        "image_path = \"/content/drive/MyDrive/BA865/Final Project/test/opneye.jpg\"\n",
        "new_img = image.load_img(image_path, target_size=(224, 224))\n",
        "img = image.img_to_array(new_img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "prediction = vgg.predict(img)\n",
        "prediction = np.argmax(prediction,axis=1)\n",
        "print(prediction)\n",
        "print(class_names[prediction[0]])\n",
        "plt.imshow(new_img)"
      ],
      "metadata": {
        "id": "dQbQoLFgFYiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using MobileNet\n",
        "img_size = 224\n",
        "img_array = cv2.imread('/content/drive/MyDrive/BA865/Final Project/test/closedeye.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2BGR)\n",
        "new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
        "X_input = np.array(new_array).reshape(1, img_size, img_size, 3)\n",
        "X_input = X_input/255.0\n",
        "prediction = model.predict(X_input)\n",
        "prediction"
      ],
      "metadata": {
        "id": "dyTkDSagsisN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/drive/MyDrive/BA865/Final Project/test/closedeye.jpg')\n",
        "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "NArc383LW0l5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}